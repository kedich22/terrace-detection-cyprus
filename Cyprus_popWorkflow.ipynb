{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyprus Terrace Detection: Feature Extraction Workflow\n",
    "\n",
    "This notebook processes satellite imagery and DEM data to extract features for terrace detection across Cyprus. The workflow includes:\n",
    "- Training data processing and feature extraction\n",
    "- Data cleaning and NaN handling for water-dominated areas\n",
    "- Parallel processing of polygon grids across the entire island\n",
    "\n",
    "**Data Sources:**\n",
    "- Sentinel-2 imagery\n",
    "- Google Earth imagery  \n",
    "- ALOS DEM\n",
    "- Land cover classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Dependencies\n",
    "\n",
    "Import required libraries for geospatial processing, parallel computing, and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core geospatial and data processing libraries\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Raster processing libraries\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.mask import mask\n",
    "import geowombat as gw\n",
    "import richdem as rd\n",
    "from pysheds.grid import Grid\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "# Parallel processing and utilities\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Polygon, mapping\n",
    "\n",
    "# Custom modules for terrace detection\n",
    "from workflow_functions import process_workflow\n",
    "import fast_glcm\n",
    "from edges_cyprus import edge_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Paths Configuration\n",
    "\n",
    "Define paths to input datasets and configure data sources for the feature extraction workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base data directory\n",
    "DATA_DIR = \"E:/Cyprus_paper_data\"\n",
    "\n",
    "# Training polygon files for initial feature extraction\n",
    "training_polygon_paths = glob.glob(f\"{DATA_DIR}/training_new_gpkg/*.gpkg\")\n",
    "\n",
    "# Core raster datasets\n",
    "landcover_raster = f\"{DATA_DIR}/mosaic_Cyprus36n.tif\"\n",
    "dem_raster = f\"{DATA_DIR}/ALOS_newCyprus.tif\"\n",
    "\n",
    "# Sentinel-2 imagery bands\n",
    "sentinel_imagery = {\n",
    "    \"blue\": f\"{DATA_DIR}/Sentinel2/blueSen.tif\",\n",
    "    \"red\": f\"{DATA_DIR}/Sentinel2/redSen.tif\", \n",
    "    \"green\": f\"{DATA_DIR}/Sentinel2/greenSen.tif\",\n",
    "    \"gray\": f\"{DATA_DIR}/Sentinel2/graySen.tif\"\n",
    "}\n",
    "\n",
    "# Google Earth imagery\n",
    "google_imagery = {\n",
    "    \"red\": f\"{DATA_DIR}/red.tif\",\n",
    "    \"gray\": f\"{DATA_DIR}/grayscale.tif\"\n",
    "}\n",
    "\n",
    "print(f\"Found {len(training_polygon_paths)} training polygon files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Data Processing\n",
    "\n",
    "Extract features from training/validation polygon grids using parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_polygon(polygon_file):\n",
    "    \"\"\"\n",
    "    Process a single polygon file through the feature extraction workflow.\n",
    "    \n",
    "    Args:\n",
    "        polygon_file (str): Path to polygon GPKG file\n",
    "    \"\"\"\n",
    "    process_workflow(polygon_file, google_imagery, sentinel_imagery, landcover_raster, dem_raster)\n",
    "\n",
    "# Configure parallel processing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(f\"Using {num_cores} CPU cores for parallel processing\")\n",
    "\n",
    "# Process all training polygon files\n",
    "print(\"Processing training polygon files...\")\n",
    "Parallel(n_jobs=num_cores)(\n",
    "    delayed(process_single_polygon)(polygon_file) \n",
    "    for polygon_file in training_polygon_paths\n",
    ")\n",
    "print(\"Training data processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning: Handle Missing Values\n",
    "\n",
    "Clean processed data by filling NaN values in topographic features for water-dominated areas (land cover class 80)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_water_dominated_polygons():\n",
    "    \"\"\"\n",
    "    Fill NaN values with 0 for topographic features in water-dominated areas.\n",
    "    Water areas (land cover = 80) naturally have undefined topographic statistics.\n",
    "    \"\"\"\n",
    "    # Get all processed polygon files\n",
    "    processed_polygon_paths = glob.glob(f\"{DATA_DIR}/grid_all_polysSen/*.gpkg\")\n",
    "    \n",
    "    # Topographic columns that should be 0 in water areas\n",
    "    topographic_columns = [\n",
    "        'range_elevation', 'std_elevation', 'percentile_10_slope',\n",
    "        'percentile_90_slope', 'mean_slope', 'std_slope', 'range_slope',\n",
    "        'range_profcurv', 'std_profcurv', 'range_plancurv', 'std_plancurv'\n",
    "    ]\n",
    "    \n",
    "    total_nans_remaining = 0\n",
    "    \n",
    "    print(f\"Cleaning {len(processed_polygon_paths)} polygon files...\")\n",
    "    \n",
    "    for polygon_file in processed_polygon_paths:\n",
    "        # Load polygon data\n",
    "        polygons = gpd.read_file(polygon_file)\n",
    "        \n",
    "        # Identify water-dominated polygons with missing topographic data\n",
    "        water_polygons_mask = (\n",
    "            (polygons['majority_landcover'] == 80) & \n",
    "            (polygons['range_elevation'].isnull())\n",
    "        )\n",
    "        \n",
    "        # Fill NaN values with 0 for water-dominated areas\n",
    "        polygons.loc[water_polygons_mask, topographic_columns] = (\n",
    "            polygons.loc[water_polygons_mask, topographic_columns].fillna(0)\n",
    "        )\n",
    "        \n",
    "        # Save cleaned data\n",
    "        polygons.to_file(polygon_file, driver='GPKG')\n",
    "        \n",
    "        # Count remaining NaN values (excluding terrace labels)\n",
    "        remaining_nans = (\n",
    "            polygons.drop(columns=['terrace'], errors='ignore')\n",
    "            .isnull().sum().sum()\n",
    "        )\n",
    "        total_nans_remaining += remaining_nans\n",
    "        \n",
    "        if remaining_nans > 0:\n",
    "            print(f\"  {os.path.basename(polygon_file)}: {remaining_nans} NaN values remaining\")\n",
    "    \n",
    "    print(f\"\\nCleaning completed. Total NaN values remaining: {total_nans_remaining}\")\n",
    "\n",
    "# Execute cleaning for training data\n",
    "clean_water_dominated_polygons()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merge Training Data\n",
    "\n",
    "Combine all processed training polygon files into a single dataset for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_training_polygons():\n",
    "    \"\"\"\n",
    "    Merge all processed training polygon files into a single GeoPackage.\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the merged training dataset\n",
    "    \"\"\"\n",
    "    # Get all processed polygon files\n",
    "    processed_files = glob.glob(f\"{DATA_DIR}/grid_all_polysSen/*.gpkg\")\n",
    "    \n",
    "    print(f\"Merging {len(processed_files)} polygon files...\")\n",
    "    \n",
    "    # Load and combine all polygon files\n",
    "    polygon_dataframes = []\n",
    "    for polygon_file in processed_files:\n",
    "        polygons = gpd.read_file(polygon_file)\n",
    "        polygon_dataframes.append(polygons)\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    merged_training_data = gpd.GeoDataFrame(\n",
    "        pd.concat(polygon_dataframes, ignore_index=True)\n",
    "    )\n",
    "    \n",
    "    # Save merged dataset\n",
    "    output_path = f\"{DATA_DIR}/TrainingCyprus_popsegs.gpkg\"\n",
    "    merged_training_data.to_file(output_path, driver=\"GPKG\")\n",
    "    \n",
    "    print(f\"Training data merged successfully!\")\n",
    "    print(f\"  Total polygons: {len(merged_training_data):,}\")\n",
    "    print(f\"  Output file: {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Execute merge\n",
    "training_data_path = merge_training_polygons()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Full Cyprus Processing\n",
    "\n",
    "Process the remaining polygon grids to cover the entire island of Cyprus for comprehensive feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all remaining polygon files for full Cyprus coverage\n",
    "cyprus_polygon_folder = f\"{DATA_DIR}/CyprusMap/polygons_all\"\n",
    "\n",
    "# Collect all GPKG files in the directory\n",
    "remaining_polygon_files = []\n",
    "for root, dirs, filenames in os.walk(cyprus_polygon_folder):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.gpkg'):\n",
    "            remaining_polygon_files.append(os.path.join(root, filename))\n",
    "\n",
    "print(f\"Found {len(remaining_polygon_files)} polygon files for full Cyprus processing\")\n",
    "print(f\"Sample files: {remaining_polygon_files[:3]}\")  # Show first 3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure parallel processing for full Cyprus\n",
    "NUM_CORES_CYPRUS = 8  # Reduced cores to prevent system overload\n",
    "\n",
    "print(f\"Processing {len(remaining_polygon_files)} polygon files using {NUM_CORES_CYPRUS} cores...\")\n",
    "\n",
    "# Process all remaining polygon files for full Cyprus coverage\n",
    "Parallel(n_jobs=NUM_CORES_CYPRUS)(\n",
    "    delayed(process_single_polygon)(polygon_file) \n",
    "    for polygon_file in remaining_polygon_files\n",
    ")\n",
    "\n",
    "print(\"Full Cyprus processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Data Cleaning\n",
    "\n",
    "Apply the same cleaning procedure to the full Cyprus dataset to handle water-dominated areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply final cleaning to all processed data\n",
    "print(\"Applying final data cleaning to all processed Cyprus data...\")\n",
    "clean_water_dominated_polygons()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WORKFLOW COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Summary:\")\n",
    "print(\"- Training data processed and merged\")\n",
    "print(\"- Full Cyprus polygon grids processed\")  \n",
    "print(\"- Water-dominated areas cleaned\")\n",
    "print(\"- All data ready for terrace detection modeling\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geodata_proc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
